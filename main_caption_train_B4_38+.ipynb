{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-tune BLIP using Hugging Face `transformers` and `datasets` 🤗\n\nThis tutorial is largely based from the [GiT tutorial](https://colab.research.google.com/drive/1HLxgrG7xZJ9FvXckNG61J72FkyrbqKAA?usp=sharing) on how to fine-tune GiT on a custom image captioning dataset. Here we will use a dummy dataset of [football players](https://huggingface.co/datasets/ybelkada/football-dataset) ⚽ that is uploaded on the Hub. The images have been manually selected together with the captions. \nCheck the 🤗 [documentation](https://huggingface.co/docs/datasets/image_dataset) on how to create and upload your own image-text dataset.","metadata":{"id":"prwiIBb1lin_"}},{"cell_type":"code","source":"# install requirements\nimport sys\n\n!pip3 install transformers==4.15.0 timm==0.4.12 fairscale==0.4.4\n# !git clone https://github.com/salesforce/BLIP\n!pip install pycocotools pycocoevalcap","metadata":{"execution":{"iopub.status.busy":"2023-04-14T01:30:14.024268Z","iopub.execute_input":"2023-04-14T01:30:14.024648Z","iopub.status.idle":"2023-04-14T01:31:30.158392Z","shell.execute_reply.started":"2023-04-14T01:30:14.024614Z","shell.execute_reply":"2023-04-14T01:31:30.157163Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting transformers==4.15.0\n  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting timm==0.4.12\n  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting fairscale==0.4.4\n  Downloading fairscale-0.4.4.tar.gz (235 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.4/235.4 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.15.0) (1.21.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.15.0) (3.9.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.15.0) (2021.11.10)\nCollecting tokenizers<0.11,>=0.10.1\n  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.15.0) (23.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.15.0) (6.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.15.0) (0.13.3)\nCollecting sacremoses\n  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.15.0) (4.11.4)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.15.0) (4.64.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.15.0) (2.28.2)\nRequirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.7/site-packages (from timm==0.4.12) (1.13.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm==0.4.12) (0.14.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.15.0) (4.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.15.0) (3.11.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.15.0) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.15.0) (1.26.14)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.15.0) (2022.12.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.15.0) (2.1.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.15.0) (1.16.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.15.0) (8.1.3)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.15.0) (1.2.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->timm==0.4.12) (9.4.0)\nBuilding wheels for collected packages: fairscale, sacremoses\n  Building wheel for fairscale (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fairscale: filename=fairscale-0.4.4-py3-none-any.whl size=292837 sha256=0a57e542ca492b783473db82ece53c2eb2022c13bdb92a4ef02a41df92674321\n  Stored in directory: /root/.cache/pip/wheels/51/77/ae/9576097dae2412e61bc5435e3df7ab24df98e9e43ff6d2c07d\n  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=2240ca83c9b6825b3ba4fd5fe3306b74d08452f5a2a89b5129a9b02d6b8cbd66\n  Stored in directory: /root/.cache/pip/wheels/5b/e0/77/05245143a5b31f65af6a21f7afd3219e9fa4896f918af45677\nSuccessfully built fairscale sacremoses\nInstalling collected packages: tokenizers, fairscale, timm, sacremoses, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.13.2\n    Uninstalling tokenizers-0.13.2:\n      Successfully uninstalled tokenizers-0.13.2\n  Attempting uninstall: timm\n    Found existing installation: timm 0.6.13\n    Uninstalling timm-0.6.13:\n      Successfully uninstalled timm-0.6.13\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.27.4\n    Uninstalling transformers-4.27.4:\n      Successfully uninstalled transformers-4.27.4\nSuccessfully installed fairscale-0.4.4 sacremoses-0.0.53 timm-0.4.12 tokenizers-0.10.3 transformers-4.15.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting pycocotools\n  Downloading pycocotools-2.0.6.tar.gz (24 kB)\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting pycocoevalcap\n  Downloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from pycocotools) (1.21.6)\nRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from pycocotools) (3.5.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (9.4.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.4)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (0.11.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (23.0)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (3.0.9)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (4.38.0)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=2.1.0->pycocotools) (4.4.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\nBuilding wheels for collected packages: pycocotools\n  Building wheel for pycocotools (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pycocotools: filename=pycocotools-2.0.6-cp37-cp37m-linux_x86_64.whl size=373910 sha256=95916c24150b4b62e987d498a7d701481704356c1bb2c5b7ec7c1ac489e4e350\n  Stored in directory: /root/.cache/pip/wheels/d0/90/d4/e9ae0a3cdbd8e0cddf6b5fe8c31774fb9bd0ae4e9754fb2314\nSuccessfully built pycocotools\nInstalling collected packages: pycocotools, pycocoevalcap\nSuccessfully installed pycocoevalcap-1.2 pycocotools-2.0.6\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# ! wget https://github.com/salesforce/BLIP/archive/refs/heads/main.zip\n# ! unzip main.zip\n# ! cp -r BLIP-main/* /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2023-04-14T01:31:30.161253Z","iopub.execute_input":"2023-04-14T01:31:30.161986Z","iopub.status.idle":"2023-04-14T01:31:30.171024Z","shell.execute_reply.started":"2023-04-14T01:31:30.161943Z","shell.execute_reply":"2023-04-14T01:31:30.170101Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# %cd /kaggle/working\n# ! git clone https://github.com/MahmoudQaid/Blip\n# %cd Blip","metadata":{"execution":{"iopub.status.busy":"2023-04-14T01:31:30.172879Z","iopub.execute_input":"2023-04-14T01:31:30.173292Z","iopub.status.idle":"2023-04-14T01:31:30.180970Z","shell.execute_reply.started":"2023-04-14T01:31:30.173246Z","shell.execute_reply":"2023-04-14T01:31:30.179936Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from tqdm.notebook import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-04-14T01:31:30.184489Z","iopub.execute_input":"2023-04-14T01:31:30.184818Z","iopub.status.idle":"2023-04-14T01:31:30.266374Z","shell.execute_reply.started":"2023-04-14T01:31:30.184790Z","shell.execute_reply":"2023-04-14T01:31:30.265406Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# train_caption.py","metadata":{}},{"cell_type":"code","source":"def u():\n    %cd /kaggle/working\n    ! rm -r Blip\n    ! git clone https://github.com/MahmoudQaid/Blip\n    %cd Blip\nu()","metadata":{"execution":{"iopub.status.busy":"2023-04-14T01:31:30.267725Z","iopub.execute_input":"2023-04-14T01:31:30.268168Z","iopub.status.idle":"2023-04-14T01:31:34.046811Z","shell.execute_reply.started":"2023-04-14T01:31:30.268130Z","shell.execute_reply":"2023-04-14T01:31:34.044918Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/kaggle/working\nrm: cannot remove 'Blip': No such file or directory\nCloning into 'Blip'...\nremote: Enumerating objects: 151, done.\u001b[K\nremote: Counting objects: 100% (151/151), done.\u001b[K\nremote: Compressing objects: 100% (124/124), done.\u001b[K\nremote: Total 151 (delta 84), reused 70 (delta 25), pack-reused 0\u001b[K\nReceiving objects: 100% (151/151), 6.95 MiB | 8.05 MiB/s, done.\nResolving deltas: 100% (84/84), done.\n/kaggle/working/Blip\n","output_type":"stream"}]},{"cell_type":"code","source":"from collections import defaultdict\nimport json,string,re\n\nmasked_word=['is','at','for','am','are','and','or','but','of']\npattern=r'\\b('+'|'.join(masked_word)+r')\\b'\n\ndef load_doc_karpathy(json_file_path,pattern=None,lower=False):\n    # this function return dictionary \n    with open(json_file_path,'r') as file:\n        data=json.loads(file.read())\n    dict_data=defaultdict(list)\n    for example in data['images']:\n        temp=[]\n        \n        for sentence in example['sentences']:\n            cap=sentence['raw']\n            if lower:\n                cap=cap.lower()\n                \n            cap=cap.translate(str.maketrans('','',string.punctuation))\n            \n            if pattern is not None:\n                cap=re.sub(pattern,'',cap)\n            cap=' '.join(cap.split())\n            if example['split']=='train':\n                dict_data[example['split']].append({'caption':cap,\n                                                    'image': example['filename'],\n                                                    'image_id': example['imgid']})\n            else:\n                temp.append(cap)\n                \n        if example['split']!='train':\n            dict_data[example['split']].append({'caption':temp,\n                                                'image': example['filename']\n                                             })\n            \n# {'train':[{'image':'name.jpg','image_id':img_id,'caption':cap}],'test':[{'image':'name.jpg','captions':[cap1,...,cap5]}],'val':[{'image':'name.jpg','captions':[cap1,...,cap5]}]}  \n    return dict_data\n\n\nKARPATHY_DATA=load_doc_karpathy(json_file_path='/kaggle/input/karpathy-splits/dataset_flickr8k.json',pattern=pattern,lower=True)   \n","metadata":{"execution":{"iopub.status.busy":"2023-04-14T01:31:34.048945Z","iopub.execute_input":"2023-04-14T01:31:34.049340Z","iopub.status.idle":"2023-04-14T01:31:34.908873Z","shell.execute_reply.started":"2023-04-14T01:31:34.049293Z","shell.execute_reply":"2023-04-14T01:31:34.907745Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def prepare_ref_caps_for_evaluate(list_dict,output_json_path):\n    references={'annotations':[],\n                'images':[]}\n    j=0\n    for i,d in enumerate(list_dict):\n        for cap in d['caption']:\n            references['annotations'].append({'image_id':d['image'],'caption':cap,'id':j})\n            j+=1\n        references['images'].append({'id':d['image']})\n    json.dump(references,open(output_json_path,'w'))\n    \n    return references\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-04-14T01:31:34.910414Z","iopub.execute_input":"2023-04-14T01:31:34.910823Z","iopub.status.idle":"2023-04-14T01:31:34.918182Z","shell.execute_reply.started":"2023-04-14T01:31:34.910781Z","shell.execute_reply":"2023-04-14T01:31:34.917140Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"len(KARPATHY_DATA['train'])","metadata":{"execution":{"iopub.status.busy":"2023-04-14T01:31:34.919815Z","iopub.execute_input":"2023-04-14T01:31:34.921045Z","iopub.status.idle":"2023-04-14T01:31:34.934168Z","shell.execute_reply.started":"2023-04-14T01:31:34.921004Z","shell.execute_reply":"2023-04-14T01:31:34.933038Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"30000"},"metadata":{}}]},{"cell_type":"code","source":"# New=defaultdict(list)\n# for i in KARPATHY_DATA:\n#     for j,d in enumerate(KARPATHY_DATA[i]):\n#         if j ==10:\n#             break\n#         New[i].append(d)\n# # d\n# KARPATHY_DATA=dict(New)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-14T01:31:34.935838Z","iopub.execute_input":"2023-04-14T01:31:34.936294Z","iopub.status.idle":"2023-04-14T01:31:34.943168Z","shell.execute_reply.started":"2023-04-14T01:31:34.936257Z","shell.execute_reply":"2023-04-14T01:31:34.942054Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# ! mkdir annotation\n# json.dump(KARPATHY_DATA['val'],open('annotation/coco_karpathy_val_gt.json','w'))\n# json.dump(KARPATHY_DATA['test'],open('annotation/coco_karpathy_test_gt.json','w'))\n# json.dump(KARPATHY_DATA['train'],open('annotation/coco_karpathy_train_gt.json','w'))","metadata":{"execution":{"iopub.status.busy":"2023-04-14T01:31:34.948699Z","iopub.execute_input":"2023-04-14T01:31:34.949018Z","iopub.status.idle":"2023-04-14T01:31:34.955802Z","shell.execute_reply.started":"2023-04-14T01:31:34.948992Z","shell.execute_reply":"2023-04-14T01:31:34.954638Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# ! pip install vit_pytorch\n# import torch\n# from vit_pytorch.vit import ViT\n\n# v = ViT(\n#     image_size = 256,\n#     patch_size = 32,\n#     num_classes = 1000,\n#     dim = 1024,\n#     depth = 6,\n#     heads = 16,\n#     mlp_dim = 2048,\n#     dropout = 0.1,\n#     emb_dropout = 0.1\n# )\n\n# # forward pass now returns predictions and the representation of the final layer\n# predictions = v(torch.randn(1, 3, 256, 256))\n# predictions.shape","metadata":{"execution":{"iopub.status.busy":"2023-04-14T01:31:34.957325Z","iopub.execute_input":"2023-04-14T01:31:34.957949Z","iopub.status.idle":"2023-04-14T01:31:34.963307Z","shell.execute_reply.started":"2023-04-14T01:31:34.957905Z","shell.execute_reply":"2023-04-14T01:31:34.962102Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"model=None","metadata":{"execution":{"iopub.status.busy":"2023-04-14T01:31:34.964972Z","iopub.execute_input":"2023-04-14T01:31:34.965407Z","iopub.status.idle":"2023-04-14T01:31:34.972530Z","shell.execute_reply.started":"2023-04-14T01:31:34.965368Z","shell.execute_reply":"2023-04-14T01:31:34.971338Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"'''\n * Copyright (c) 2022, salesforce.com, inc.\n * All rights reserved.\n * SPDX-License-Identifier: BSD-3-Clause\n * For full license text, see LICENSE.txt file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n * By Junnan Li\n'''\nimport argparse\nimport os\nimport ruamel_yaml as yaml\nimport numpy as np\nimport random\nimport time\nimport datetime\nimport json\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\nimport torch.distributed as dist\nfrom torch.utils.data import DataLoader\n\nfrom models.blip import blip_decoder\nimport utils\nfrom utils import cosine_lr_schedule\nfrom data import create_dataset, create_sampler, create_loader\nfrom data.utils import save_result, coco_caption_eval\n\n\n# dist.init_process_group(backend='nccl',world_size=1, init_method='env://',rank=0)\n\n\ndef train(model, data_loader, optimizer, epoch, device):\n    # train\n    model.train()  \n    \n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n    metric_logger.add_meter('loss', utils.SmoothedValue(window_size=1, fmt='{value:.4f}'))\n    header = 'Train Caption Epoch: [{}]'.format(epoch)\n    print_freq = 50\n\n    for i, (image, caption, _) in enumerate(metric_logger.log_every(data_loader, print_freq, header)):\n        image = image.to(device)       \n        \n        loss = model(image, caption)      \n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()    \n        \n        metric_logger.update(loss=loss.item())\n        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n\n    # gather the stats from all processes\n    metric_logger.synchronize_between_processes()\n    print(\"Averaged stats:\", metric_logger.global_avg())     \n    return {k: \"{:.3f}\".format(meter.global_avg) for k, meter in metric_logger.meters.items()}  \n\n\n@torch.no_grad()\ndef evaluate(model, data_loader, device, config):\n    # evaluate\n    model.eval() \n    \n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    header = 'Caption generation:'\n    print_freq = 10\n\n    result = []\n    for image, image_id in metric_logger.log_every(data_loader, print_freq, header): \n        \n        image = image.to(device)       \n        \n        captions = model.generate(image, sample=False, num_beams=config['num_beams'], max_length=config['max_length'], \n                                  min_length=config['min_length'])\n        \n        for caption, img_id in zip(captions, image_id):\n#             result.append({\"image_id\": img_id.item(), \"caption\": caption})\n            result.append({\"image_id\": img_id, \"caption\": caption})\n  \n    return result\n\n\ndef main(args, config,model=None):\n    utils.init_distributed_mode(args)    \n    \n    device = torch.device(args.device)\n\n    # fix the seed for reproducibility\n    seed = args.seed + utils.get_rank()\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    cudnn.benchmark = True\n\n    #### Dataset #### \n    print(\"Creating captioning dataset\")\n    train_dataset, val_dataset, test_dataset = create_dataset('caption_coco', config,annot_dict=KARPATHY_DATA)  \n#     train_dataset, val_dataset, test_dataset = create_dataset('caption_coco', config)  \n\n    if args.distributed:\n        num_tasks = utils.get_world_size()\n        global_rank = utils.get_rank()            \n        samplers = create_sampler([train_dataset,val_dataset,test_dataset], [True,False,False], num_tasks, global_rank)         \n    else:\n        samplers = [None, None, None]\n    \n    train_loader, val_loader, test_loader = create_loader([train_dataset, val_dataset, test_dataset],samplers,\n                                                          batch_size=[config['batch_size']]*3,num_workers=[4,4,4],\n                                                          is_trains=[True, False, False], collate_fns=[None,None,None])         \n\n    #### Model #### \n    print(\"Creating model\")\n    if config['load_checkpoint']:\n        model = blip_decoder(pretrained='', image_size=config['image_size'], vit=config['vit'], \n                               vit_grad_ckpt=config['vit_grad_ckpt'], vit_ckpt_layer=config['vit_ckpt_layer'], \n                               prompt=config['prompt'])\n        checkpoint=torch.load(config['checkpoint'])\n        model.load_state_dict(checkpoint['model'])\n    elif model is None:\n        model = blip_decoder(pretrained=config['pretrained'], image_size=config['image_size'], vit=config['vit'], \n                               vit_grad_ckpt=config['vit_grad_ckpt'], vit_ckpt_layer=config['vit_ckpt_layer'], \n                               prompt=config['prompt'])\n        for param in model.visual_encoder.parameters():\n            param.requires_grad=False\n\n    model = model.to(device)   \n    model_without_ddp = model\n    if args.distributed:\n        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n        model_without_ddp = model.module    \n    \n    optimizer = torch.optim.AdamW(params=model.parameters(), lr=config['init_lr'], weight_decay=config['weight_decay'])\n    if config['load_checkpoint']:\n        optimizer.load_state_dict(checkpoint['optimizer'])\n    best = 0\n    best_epoch = 0\n\n    print(\"Start training\")\n    start_time = time.time()    \n    for epoch in range(0, config['max_epoch']):\n        if not args.evaluate:        \n            if args.distributed:\n                train_loader.sampler.set_epoch(epoch)\n                \n            cosine_lr_schedule(optimizer, epoch, config['max_epoch'], config['init_lr'], config['min_lr'])\n            train_stats = train(model, train_loader, optimizer, epoch, device) \n        val_result = evaluate(model_without_ddp, val_loader, device, config)\n        val_result_file = save_result(val_result, args.result_dir, 'val_epoch%d'%epoch, remove_duplicate='image_id')        \n    \n        test_result = evaluate(model_without_ddp, test_loader, device, config)  \n        test_result_file = save_result(test_result, args.result_dir, 'test_epoch%d'%epoch, remove_duplicate='image_id')  \n        \n        if utils.is_main_process():\n            prepare_ref_caps_for_evaluate(KARPATHY_DATA['val'],config['ann_root']+'/coco_karpathy_val_gt.json')\n            prepare_ref_caps_for_evaluate(KARPATHY_DATA['test'],config['ann_root']+'/coco_karpathy_test_gt.json')\n            coco_val = coco_caption_eval(config['ann_root'],val_result_file,'val')\n            coco_test = coco_caption_eval(config['ann_root'],test_result_file,'test')\n            if args.evaluate:            \n                log_stats = {**{f'val_{k}': v for k, v in coco_val.eval.items()},\n                             **{f'test_{k}': v for k, v in coco_test.eval.items()},                       \n                            }\n                with open(os.path.join(args.output_dir, \"evaluate.txt\"),\"a\") as f:\n                    f.write(json.dumps(log_stats) + \"\\n\")                   \n            else:             \n                save_obj = {\n                    'model': model_without_ddp.state_dict(),\n                    'optimizer': optimizer.state_dict(),\n                    'config': config,\n                    'epoch': epoch,\n                }\n\n                if coco_val.eval['CIDEr'] + coco_val.eval['Bleu_4'] > best:\n                    best = coco_val.eval['CIDEr'] + coco_val.eval['Bleu_4']\n                    best_epoch = epoch                \n                    torch.save(save_obj, os.path.join(args.output_dir, 'checkpoint_best.pth')) \n                    \n                log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n                             **{f'val_{k}': v for k, v in coco_val.eval.items()},\n                             **{f'test_{k}': v for k, v in coco_test.eval.items()},                       \n                             'epoch': epoch,\n                             'best_epoch': best_epoch,\n                            }\n                with open(os.path.join(args.output_dir, \"log.txt\"),\"a\") as f:\n                    f.write(json.dumps(log_stats) + \"\\n\")     \n                    \n        if args.evaluate: \n            break\n#         dist.barrier()     \n\n    total_time = time.time() - start_time\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n    print('Training time {}'.format(total_time_str)) \n    return model\nclass Args(dict):\n    __setattr__ = dict.__setitem__\n    __getattr__ = dict.__getitem__\n\nargs = {\n    'config':'./configs/caption_coco.yaml',\n    'output_dir':'output/Caption_coco' ,\n    'evaluate':False ,\n    'device':'cuda',\n    'seed':42,\n    'world_size':1,  \n    'dist_url':'env://',\n    'distributed':False\n}\nargs = Args(args) # dict2object\n\nif __name__ == '__main__':\n#     parser = argparse.ArgumentParser()\n#     parser.add_argument('--config', default='./configs/caption_coco.yaml')\n#     parser.add_argument('--output_dir', default='output/Caption_coco')        \n#     parser.add_argument('--evaluate', action='store_true')    \n#     parser.add_argument('--device', default='cuda')\n#     parser.add_argument('--seed', default=42, type=int)\n#     parser.add_argument('--world_size', default=1, type=int, help='number of distributed processes')    \n#     parser.add_argument('--dist_url', default='env://', help='url used to set up distributed training')\n#     parser.add_argument('--distributed', default=True, type=bool)\n#     args = parser.parse_args()\n\n#     config = yaml.load(open(args.config, 'r'), Loader=yaml.Loader)\n    config={\n     'image_root': '/kaggle/input/flickr8k/Images',\n#      'image_root':'/kaggle/input/coco2014',\n     'ann_root': 'annotations',\n     'coco_gt_root': 'annotation/coco_gt',\n#      'pretrained':''\n     'pretrained': 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_caption_capfilt_large.pth', # vit: base\n#      'pretrained': 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth',  # vit: large\n     'vit': 'base',\n#      'vit': 'large',\n     'vit_grad_ckpt': False,\n     'vit_ckpt_layer': 0,\n     'batch_size': 16,\n     'init_lr': 1e-05,\n     'image_size': 384,\n     'max_length': 20,\n     'min_length': 5,\n     'num_beams': 3,\n     'prompt': 'a picture of ',\n     'weight_decay': 0.05,\n     'min_lr': 0,\n     'max_epoch': 1,\n     'checkpoint':'/kaggle/input/blip-temp/Blip/output/Caption_coco/checkpoint_best.pth',\n     'load_checkpoint':False\n    }\n    args.result_dir = os.path.join(args.output_dir, 'result')\n\n    Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n    Path(args.result_dir).mkdir(parents=True, exist_ok=True)\n    Path(config['ann_root']).mkdir(parents=True, exist_ok=True)\n    \n    yaml.dump(config, open(os.path.join(args.output_dir, 'config.yaml'), 'w'))    \n\n    model=main(args,config,model)","metadata":{"execution":{"iopub.status.busy":"2023-04-14T01:37:31.763609Z","iopub.execute_input":"2023-04-14T01:37:31.764053Z","iopub.status.idle":"2023-04-14T02:08:09.217417Z","shell.execute_reply.started":"2023-04-14T01:37:31.764017Z","shell.execute_reply":"2023-04-14T02:08:09.215692Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Not using distributed mode\nCreating captioning dataset\nCreating model\nload checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_caption_capfilt_large.pth\nStart training\nTrain Caption Epoch: [0]  [   0/1875]  eta: 0:53:20  lr: 0.000010  loss: 4.0479  time: 1.7068  data: 1.1439  max mem: 13352\nTrain Caption Epoch: [0]  [  50/1875]  eta: 0:16:06  lr: 0.000010  loss: 3.2966  time: 0.5105  data: 0.0002  max mem: 13352\nTrain Caption Epoch: [0]  [ 100/1875]  eta: 0:15:15  lr: 0.000010  loss: 3.5283  time: 0.5021  data: 0.0002  max mem: 13352\nTrain Caption Epoch: [0]  [ 150/1875]  eta: 0:14:42  lr: 0.000010  loss: 3.2327  time: 0.5001  data: 0.0002  max mem: 13352\nTrain Caption Epoch: [0]  [ 200/1875]  eta: 0:14:13  lr: 0.000010  loss: 3.0106  time: 0.5010  data: 0.0002  max mem: 13352\nTrain Caption Epoch: [0]  [ 250/1875]  eta: 0:13:46  lr: 0.000010  loss: 3.1243  time: 0.5130  data: 0.0002  max mem: 13352\nTrain Caption Epoch: [0]  [ 300/1875]  eta: 0:13:19  lr: 0.000010  loss: 3.3374  time: 0.5055  data: 0.0002  max mem: 13352\nTrain Caption Epoch: [0]  [ 350/1875]  eta: 0:12:52  lr: 0.000010  loss: 3.4099  time: 0.5018  data: 0.0002  max mem: 13352\nTrain Caption Epoch: [0]  [ 400/1875]  eta: 0:12:26  lr: 0.000010  loss: 3.2751  time: 0.5022  data: 0.0002  max mem: 13352\nTrain Caption Epoch: [0]  [ 450/1875]  eta: 0:12:00  lr: 0.000010  loss: 3.0667  time: 0.5006  data: 0.0002  max mem: 13352\nTrain Caption Epoch: [0]  [ 500/1875]  eta: 0:11:34  lr: 0.000010  loss: 3.1504  time: 0.5065  data: 0.0002  max mem: 13352\nTrain Caption Epoch: [0]  [ 550/1875]  eta: 0:11:09  lr: 0.000010  loss: 3.0212  time: 0.5012  data: 0.0002  max mem: 13352\nTrain Caption Epoch: [0]  [ 600/1875]  eta: 0:10:43  lr: 0.000010  loss: 3.2565  time: 0.4996  data: 0.0002  max mem: 13352\nTrain Caption Epoch: [0]  [ 650/1875]  eta: 0:10:18  lr: 0.000010  loss: 3.5711  time: 0.5041  data: 0.0002  max mem: 13352\nTrain Caption Epoch: [0]  [ 700/1875]  eta: 0:09:53  lr: 0.000010  loss: 2.9299  time: 0.5031  data: 0.0002  max mem: 13352\nTrain Caption Epoch: [0]  [ 750/1875]  eta: 0:09:28  lr: 0.000010  loss: 3.0740  time: 0.5098  data: 0.0002  max mem: 13352\nTrain Caption Epoch: [0]  [ 800/1875]  eta: 0:09:02  lr: 0.000010  loss: 3.2044  time: 0.5042  data: 0.0002  max mem: 13352\nTrain Caption Epoch: [0]  [ 850/1875]  eta: 0:08:37  lr: 0.000010  loss: 3.2977  time: 0.5039  data: 0.0002  max mem: 13352\nTrain Caption Epoch: [0]  [ 900/1875]  eta: 0:08:12  lr: 0.000010  loss: 3.3196  time: 0.4992  data: 0.0002  max mem: 13352\nTrain Caption Epoch: [0]  [ 950/1875]  eta: 0:07:47  lr: 0.000010  loss: 3.1659  time: 0.4990  data: 0.0002  max mem: 13352\nTrain Caption Epoch: [0]  [1000/1875]  eta: 0:07:21  lr: 0.000010  loss: 3.4199  time: 0.5129  data: 0.0002  max mem: 13352\nTrain Caption Epoch: [0]  [1050/1875]  eta: 0:06:56  lr: 0.000010  loss: 3.1792  time: 0.5014  data: 0.0002  max mem: 13352\nTrain Caption Epoch: [0]  [1100/1875]  eta: 0:06:31  lr: 0.000010  loss: 3.0857  time: 0.5027  data: 0.0002  max mem: 13352\nTrain Caption Epoch: [0]  [1150/1875]  eta: 0:06:05  lr: 0.000010  loss: 3.2112  time: 0.5037  data: 0.0002  max mem: 13352\nTrain Caption Epoch: [0]  [1200/1875]  eta: 0:05:40  lr: 0.000010  loss: 3.0785  time: 0.5045  data: 0.0003  max mem: 13352\nTrain Caption Epoch: [0]  [1250/1875]  eta: 0:05:15  lr: 0.000010  loss: 2.9532  time: 0.5088  data: 0.0004  max mem: 13352\nTrain Caption Epoch: [0]  [1300/1875]  eta: 0:04:50  lr: 0.000010  loss: 2.8466  time: 0.5046  data: 0.0002  max mem: 13352\nTrain Caption Epoch: [0]  [1350/1875]  eta: 0:04:24  lr: 0.000010  loss: 3.3178  time: 0.5000  data: 0.0002  max mem: 13352\nTrain Caption Epoch: [0]  [1400/1875]  eta: 0:03:59  lr: 0.000010  loss: 3.0458  time: 0.5011  data: 0.0002  max mem: 13352\nTrain Caption Epoch: [0]  [1450/1875]  eta: 0:03:34  lr: 0.000010  loss: 3.1011  time: 0.5104  data: 0.0008  max mem: 13352\nTrain Caption Epoch: [0]  [1500/1875]  eta: 0:03:09  lr: 0.000010  loss: 2.9734  time: 0.5054  data: 0.0008  max mem: 13352\nTrain Caption Epoch: [0]  [1550/1875]  eta: 0:02:44  lr: 0.000010  loss: 3.1325  time: 0.5076  data: 0.0002  max mem: 13352\nTrain Caption Epoch: [0]  [1600/1875]  eta: 0:02:18  lr: 0.000010  loss: 3.0632  time: 0.5010  data: 0.0001  max mem: 13352\nTrain Caption Epoch: [0]  [1650/1875]  eta: 0:01:53  lr: 0.000010  loss: 3.3577  time: 0.4998  data: 0.0002  max mem: 13352\nTrain Caption Epoch: [0]  [1700/1875]  eta: 0:01:28  lr: 0.000010  loss: 3.2476  time: 0.5035  data: 0.0007  max mem: 13352\nTrain Caption Epoch: [0]  [1750/1875]  eta: 0:01:03  lr: 0.000010  loss: 2.9538  time: 0.5118  data: 0.0005  max mem: 13352\nTrain Caption Epoch: [0]  [1800/1875]  eta: 0:00:37  lr: 0.000010  loss: 3.0823  time: 0.4995  data: 0.0002  max mem: 13352\nTrain Caption Epoch: [0]  [1850/1875]  eta: 0:00:12  lr: 0.000010  loss: 3.1099  time: 0.4990  data: 0.0002  max mem: 13352\nTrain Caption Epoch: [0]  [1874/1875]  eta: 0:00:00  lr: 0.000010  loss: 2.8171  time: 0.4980  data: 0.0003  max mem: 13352\nTrain Caption Epoch: [0] Total time: 0:15:45 (0.5045 s / it)\nAveraged stats: lr: 0.0000  loss: 3.2157\nCaption generation:  [ 0/63]  eta: 0:12:51    time: 12.2534  data: 1.0285  max mem: 13352\nCaption generation:  [10/63]  eta: 0:02:59    time: 3.3820  data: 0.0938  max mem: 13352\nCaption generation:  [20/63]  eta: 0:02:06    time: 2.4837  data: 0.0003  max mem: 13352\nCaption generation:  [30/63]  eta: 0:01:32    time: 2.4673  data: 0.0003  max mem: 13352\nCaption generation:  [40/63]  eta: 0:01:02    time: 2.4691  data: 0.0004  max mem: 13352\nCaption generation:  [50/63]  eta: 0:00:34    time: 2.4746  data: 0.0003  max mem: 13352\nCaption generation:  [60/63]  eta: 0:00:07    time: 2.4595  data: 0.0002  max mem: 13352\nCaption generation:  [62/63]  eta: 0:00:02    time: 2.4221  data: 0.0002  max mem: 13352\nCaption generation: Total time: 0:02:44 (2.6176 s / it)\nresult file saved to output/Caption_coco/result/val_epoch0.json\nCaption generation:  [ 0/63]  eta: 0:03:35    time: 3.4254  data: 0.8232  max mem: 13352\nCaption generation:  [10/63]  eta: 0:02:15    time: 2.5504  data: 0.0751  max mem: 13352\nCaption generation:  [20/63]  eta: 0:01:48    time: 2.4672  data: 0.0003  max mem: 13352\nCaption generation:  [30/63]  eta: 0:01:22    time: 2.4588  data: 0.0002  max mem: 13352\nCaption generation:  [40/63]  eta: 0:00:57    time: 2.4623  data: 0.0002  max mem: 13352\nCaption generation:  [50/63]  eta: 0:00:32    time: 2.4759  data: 0.0003  max mem: 13352\nCaption generation:  [60/63]  eta: 0:00:07    time: 2.4647  data: 0.0003  max mem: 13352\nCaption generation:  [62/63]  eta: 0:00:02    time: 2.4061  data: 0.0003  max mem: 13352\nCaption generation: Total time: 0:02:35 (2.4647 s / it)\nresult file saved to output/Caption_coco/result/test_epoch0.json\nloading annotations into memory...\nDone (t=0.01s)\ncreating index...\nindex created!\nLoading and preparing results...\nDONE (t=0.00s)\ncreating index...\nindex created!\ntokenization...\n","output_type":"stream"},{"name":"stderr","text":"PTBTokenizer tokenized 55487 tokens at 177154.81 tokens per second.\nPTBTokenizer tokenized 9904 tokens at 45610.27 tokens per second.\n","output_type":"stream"},{"name":"stdout","text":"setting up scorers...\nDownloading stanford-corenlp-3.6.0 for SPICE ...\nProgress: 384.5M / 384.5M (100.0%)\nExtracting stanford-corenlp-3.6.0 ...\nDone.\ncomputing Bleu score...\n{'testlen': 8905, 'reflen': 8929, 'guess': [8905, 7905, 6905, 5905], 'correct': [6972, 4004, 1986, 874]}\nratio: 0.9973121290176954\nBleu_1: 0.781\nBleu_2: 0.628\nBleu_3: 0.484\nBleu_4: 0.359\ncomputing METEOR score...\nMETEOR: 0.284\ncomputing Rouge score...\nROUGE_L: 0.601\ncomputing CIDEr score...\nCIDEr: 1.105\ncomputing SPICE score...\n","output_type":"stream"},{"name":"stderr","text":"WARNING: An illegal reflective access operation has occurred\nWARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/opt/conda/lib/python3.7/site-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value\nWARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\nWARNING: All illegal access operations will be denied in a future release\nParsing reference captions\nInitiating Stanford parsing pipeline\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \ndone [1.2 sec].\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\nLoading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [2.1 sec].\nLoading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [1.1 sec].\nLoading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.8 sec].\nThreads( StanfordCoreNLP ) [03:08.669 minutes]\nParsing test captions\nThreads( StanfordCoreNLP ) [26.290 seconds]\nWarning: Nashorn engine is planned to be removed from a future JDK release\n","output_type":"stream"},{"name":"stdout","text":"SPICE evaluation took: 3.836 min\nSPICE: 0.220\nBleu_1: 0.781\nBleu_2: 0.628\nBleu_3: 0.484\nBleu_4: 0.359\nMETEOR: 0.284\nROUGE_L: 0.601\nCIDEr: 1.105\nSPICE: 0.220\nloading annotations into memory...\nDone (t=0.01s)\ncreating index...\nindex created!\nLoading and preparing results...\nDONE (t=0.00s)\ncreating index...\nindex created!\ntokenization...\n","output_type":"stream"},{"name":"stderr","text":"PTBTokenizer tokenized 55218 tokens at 179894.08 tokens per second.\nPTBTokenizer tokenized 9915 tokens at 51629.67 tokens per second.\n","output_type":"stream"},{"name":"stdout","text":"setting up scorers...\ncomputing Bleu score...\n{'testlen': 8916, 'reflen': 8895, 'guess': [8916, 7916, 6916, 5916], 'correct': [7044, 4218, 2211, 1054]}\nratio: 1.0023608768970205\nBleu_1: 0.790\nBleu_2: 0.649\nBleu_3: 0.512\nBleu_4: 0.394\ncomputing METEOR score...\nMETEOR: 0.298\ncomputing Rouge score...\nROUGE_L: 0.613\ncomputing CIDEr score...\nCIDEr: 1.175\ncomputing SPICE score...\n","output_type":"stream"},{"name":"stderr","text":"WARNING: An illegal reflective access operation has occurred\nWARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/opt/conda/lib/python3.7/site-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value\nWARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\nWARNING: All illegal access operations will be denied in a future release\nParsing reference captions\nInitiating Stanford parsing pipeline\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \ndone [0.9 sec].\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\nLoading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.7 sec].\nLoading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.9 sec].\nLoading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.8 sec].\nThreads( StanfordCoreNLP ) [03:07.513 minutes]\nParsing test captions\nThreads( StanfordCoreNLP ) [24.855 seconds]\nWarning: Nashorn engine is planned to be removed from a future JDK release\n","output_type":"stream"},{"name":"stdout","text":"SPICE evaluation took: 3.765 min\nSPICE: 0.226\nBleu_1: 0.790\nBleu_2: 0.649\nBleu_3: 0.512\nBleu_4: 0.394\nMETEOR: 0.298\nROUGE_L: 0.613\nCIDEr: 1.175\nSPICE: 0.226\nTraining time 0:30:28\n","output_type":"stream"}]},{"cell_type":"code","source":"# from torchsummary import summary\n# summary(model,)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert(False)","metadata":{"execution":{"iopub.status.busy":"2023-04-14T01:33:57.172506Z","iopub.status.idle":"2023-04-14T01:33:57.172890Z","shell.execute_reply.started":"2023-04-14T01:33:57.172705Z","shell.execute_reply":"2023-04-14T01:33:57.172724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from glob import glob\nglob('output/Caption_coco/result/*')","metadata":{"execution":{"iopub.status.busy":"2023-04-14T01:33:57.174364Z","iopub.status.idle":"2023-04-14T01:33:57.175527Z","shell.execute_reply.started":"2023-04-14T01:33:57.175231Z","shell.execute_reply":"2023-04-14T01:33:57.175261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-04-14T01:33:57.177518Z","iopub.status.idle":"2023-04-14T01:33:57.178015Z","shell.execute_reply.started":"2023-04-14T01:33:57.177759Z","shell.execute_reply":"2023-04-14T01:33:57.177795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# chpt['model'].keys()","metadata":{"execution":{"iopub.status.busy":"2023-04-14T01:33:57.180290Z","iopub.status.idle":"2023-04-14T01:33:57.181518Z","shell.execute_reply.started":"2023-04-14T01:33:57.181214Z","shell.execute_reply":"2023-04-14T01:33:57.181243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visual_dict.keys()","metadata":{"execution":{"iopub.status.busy":"2023-04-14T01:33:57.182827Z","iopub.status.idle":"2023-04-14T01:33:57.183962Z","shell.execute_reply.started":"2023-04-14T01:33:57.183711Z","shell.execute_reply":"2023-04-14T01:33:57.183736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch \nfrom torchvision.datasets.utils import download_url\ndownload_url('https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth','/kaggle/working')\nchpt=torch.load('/kaggle/working/model_large_caption.pth')\nvisual_dict={}\nfor i in chpt['model']:\n    if i.split('.')[0]=='visual_encoder':\n        visual_dict[i[len('visual_encoder.'):]]=chpt['model'][i]\nmodel.visual_encoder.load_state_dict(visual_dict)","metadata":{"execution":{"iopub.status.busy":"2023-04-14T01:33:57.192167Z","iopub.status.idle":"2023-04-14T01:33:57.192748Z","shell.execute_reply.started":"2023-04-14T01:33:57.192494Z","shell.execute_reply":"2023-04-14T01:33:57.192521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-04-14T01:33:57.194802Z","iopub.status.idle":"2023-04-14T01:33:57.195299Z","shell.execute_reply.started":"2023-04-14T01:33:57.195031Z","shell.execute_reply":"2023-04-14T01:33:57.195056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# download_url('https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_caption_capfilt_large.pth','/kaggle/working')\n# chpt2=torch.load('/kaggle/working/model_base_caption_capfilt_large.pth')","metadata":{"execution":{"iopub.status.busy":"2023-04-14T01:33:57.196998Z","iopub.status.idle":"2023-04-14T01:33:57.197519Z","shell.execute_reply.started":"2023-04-14T01:33:57.197233Z","shell.execute_reply":"2023-04-14T01:33:57.197258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # len(chpt2['model'].keys()),len(chpt['model'].keys())\n# for i,key in enumerate(list(chpt['model'].keys())):\n#     if key not in list(chpt2['model'].keys()):\n#         print(i,\" : \",key)","metadata":{"execution":{"iopub.status.busy":"2023-04-14T01:33:57.199149Z","iopub.status.idle":"2023-04-14T01:33:57.200856Z","shell.execute_reply.started":"2023-04-14T01:33:57.200582Z","shell.execute_reply":"2023-04-14T01:33:57.200609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert(False)","metadata":{"execution":{"iopub.status.busy":"2023-04-14T01:33:57.202360Z","iopub.status.idle":"2023-04-14T01:33:57.203190Z","shell.execute_reply.started":"2023-04-14T01:33:57.202925Z","shell.execute_reply":"2023-04-14T01:33:57.202951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"KARPATHY_DATA['val'][0],KARPATHY_DATA['train'][0]","metadata":{"execution":{"iopub.status.busy":"2023-04-14T01:33:57.204722Z","iopub.status.idle":"2023-04-14T01:33:57.205579Z","shell.execute_reply.started":"2023-04-14T01:33:57.205294Z","shell.execute_reply":"2023-04-14T01:33:57.205322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef eval_train_split():\n    train_kar=[]\n    train_img=[]\n    for i in KARPATHY_DATA['train']:\n        if i['image'] in train_img:\n            continue\n        train_img.append(i['image'])\n        train_kar.append({'image':i['image'],'caption':[i['caption']]})\n\n    train_dataset, val_dataset, test_dataset = create_dataset('caption_coco', config,annot_dict={'val':train_kar,'train':KARPATHY_DATA['train'],'test':KARPATHY_DATA['val']})  \n\n\n    samplers = [None, None, None]\n\n    train_loader, val_loader, test_loader = create_loader([train_dataset, val_dataset, test_dataset],samplers,\n                                                          batch_size=[config['batch_size']]*3,num_workers=[4,4,4],\n                                                          is_trains=[True, False, False], collate_fns=[None,None,None])         \n    train_result=evaluate(model, val_loader, device, config)\n    json.dump(train_result,open('/kaggle/working/train_result.json','w'))\n    print(train_result[0])\n    \n    \neval_train_split()\n","metadata":{"execution":{"iopub.status.busy":"2023-04-14T02:08:40.988161Z","iopub.execute_input":"2023-04-14T02:08:40.988593Z","iopub.status.idle":"2023-04-14T02:24:11.645142Z","shell.execute_reply.started":"2023-04-14T02:08:40.988559Z","shell.execute_reply":"2023-04-14T02:24:11.643769Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Caption generation:  [  0/375]  eta: 0:21:12    time: 3.3924  data: 0.9143  max mem: 13352\nCaption generation:  [ 10/375]  eta: 0:15:29    time: 2.5462  data: 0.0835  max mem: 13352\nCaption generation:  [ 20/375]  eta: 0:14:54    time: 2.4770  data: 0.0004  max mem: 13352\nCaption generation:  [ 30/375]  eta: 0:14:24    time: 2.4858  data: 0.0003  max mem: 13352\nCaption generation:  [ 40/375]  eta: 0:13:57    time: 2.4772  data: 0.0003  max mem: 13352\nCaption generation:  [ 50/375]  eta: 0:13:30    time: 2.4759  data: 0.0003  max mem: 13352\nCaption generation:  [ 60/375]  eta: 0:13:05    time: 2.4816  data: 0.0002  max mem: 13352\nCaption generation:  [ 70/375]  eta: 0:12:40    time: 2.4916  data: 0.0003  max mem: 13352\nCaption generation:  [ 80/375]  eta: 0:12:15    time: 2.4943  data: 0.0003  max mem: 13352\nCaption generation:  [ 90/375]  eta: 0:11:49    time: 2.4715  data: 0.0003  max mem: 13352\nCaption generation:  [100/375]  eta: 0:11:23    time: 2.4508  data: 0.0003  max mem: 13352\nCaption generation:  [110/375]  eta: 0:10:58    time: 2.4658  data: 0.0003  max mem: 13352\nCaption generation:  [120/375]  eta: 0:10:33    time: 2.4727  data: 0.0003  max mem: 13352\nCaption generation:  [130/375]  eta: 0:10:08    time: 2.4643  data: 0.0003  max mem: 13352\nCaption generation:  [140/375]  eta: 0:09:43    time: 2.4712  data: 0.0002  max mem: 13352\nCaption generation:  [150/375]  eta: 0:09:18    time: 2.4777  data: 0.0002  max mem: 13352\nCaption generation:  [160/375]  eta: 0:08:53    time: 2.4773  data: 0.0003  max mem: 13352\nCaption generation:  [170/375]  eta: 0:08:28    time: 2.4846  data: 0.0003  max mem: 13352\nCaption generation:  [180/375]  eta: 0:08:04    time: 2.4922  data: 0.0003  max mem: 13352\nCaption generation:  [190/375]  eta: 0:07:39    time: 2.4847  data: 0.0002  max mem: 13352\nCaption generation:  [200/375]  eta: 0:07:14    time: 2.4813  data: 0.0002  max mem: 13352\nCaption generation:  [210/375]  eta: 0:06:48    time: 2.4472  data: 0.0003  max mem: 13352\nCaption generation:  [220/375]  eta: 0:06:24    time: 2.4498  data: 0.0003  max mem: 13352\nCaption generation:  [230/375]  eta: 0:05:59    time: 2.4910  data: 0.0003  max mem: 13352\nCaption generation:  [240/375]  eta: 0:05:34    time: 2.4757  data: 0.0003  max mem: 13352\nCaption generation:  [250/375]  eta: 0:05:09    time: 2.4754  data: 0.0002  max mem: 13352\nCaption generation:  [260/375]  eta: 0:04:44    time: 2.4706  data: 0.0002  max mem: 13352\nCaption generation:  [270/375]  eta: 0:04:20    time: 2.4718  data: 0.0003  max mem: 13352\nCaption generation:  [280/375]  eta: 0:03:55    time: 2.4857  data: 0.0004  max mem: 13352\nCaption generation:  [290/375]  eta: 0:03:30    time: 2.4686  data: 0.0003  max mem: 13352\nCaption generation:  [300/375]  eta: 0:03:05    time: 2.4562  data: 0.0002  max mem: 13352\nCaption generation:  [310/375]  eta: 0:02:40    time: 2.4514  data: 0.0002  max mem: 13352\nCaption generation:  [320/375]  eta: 0:02:16    time: 2.4641  data: 0.0003  max mem: 13352\nCaption generation:  [330/375]  eta: 0:01:51    time: 2.4712  data: 0.0003  max mem: 13352\nCaption generation:  [340/375]  eta: 0:01:26    time: 2.4707  data: 0.0002  max mem: 13352\nCaption generation:  [350/375]  eta: 0:01:01    time: 2.4769  data: 0.0002  max mem: 13352\nCaption generation:  [360/375]  eta: 0:00:37    time: 2.4626  data: 0.0002  max mem: 13352\nCaption generation:  [370/375]  eta: 0:00:12    time: 2.4619  data: 0.0002  max mem: 13352\nCaption generation:  [374/375]  eta: 0:00:02    time: 2.4610  data: 0.0002  max mem: 13352\nCaption generation: Total time: 0:15:28 (2.4756 s / it)\n{'image_id': '2513260012_03d33305cf.jpg', 'caption': 'two dogs running in the snow'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Try model generation","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimport requests\nimport torch\nfrom torchvision import transforms\nfrom torchvision.transforms.functional import InterpolationMode\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nimage_size = 384\ndef load_demo_image(image_size,device): \n    raw_image = Image.open(img_url).convert('RGB')   \n\n    w,h = raw_image.size\n    display(raw_image.resize((w//5,h//5)))\n    \n    transform = transforms.Compose([\n        transforms.Resize((image_size,image_size),interpolation=InterpolationMode.BICUBIC),\n        transforms.ToTensor(),\n        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n        ]) \n    image = transform(raw_image).unsqueeze(0).to(device)   \n    return image\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-14T01:33:57.209513Z","iopub.status.idle":"2023-04-14T01:33:57.210349Z","shell.execute_reply.started":"2023-04-14T01:33:57.210070Z","shell.execute_reply":"2023-04-14T01:33:57.210096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# beam search\nimg_url=\"/kaggle/input/flickr8k/Images/\"+KARPATHY_DATA['test'][8]['image']\nimage=load_demo_image(image_size,device)\ncaption = model.generate(image, sample=False, num_beams=3, max_length=20, min_length=5) \n# nucleus sampling\n# caption = model.generate(image, sample=True, top_p=0.9, max_length=20, min_length=5) \nprint('caption: '+caption[0])\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-14T01:33:57.211914Z","iopub.status.idle":"2023-04-14T01:33:57.212750Z","shell.execute_reply.started":"2023-04-14T01:33:57.212491Z","shell.execute_reply":"2023-04-14T01:33:57.212518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert(False)","metadata":{"execution":{"iopub.status.busy":"2023-04-14T01:33:57.214251Z","iopub.status.idle":"2023-04-14T01:33:57.215111Z","shell.execute_reply.started":"2023-04-14T01:33:57.214855Z","shell.execute_reply":"2023-04-14T01:33:57.214882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# HappyTT ","metadata":{}},{"cell_type":"code","source":"from collections import defaultdict\nimport json,string,re\n\nmasked_word=['is','at','for','am','are','and','or','but','of']\npattern=r'\\b('+'|'.join(masked_word)+r')\\b'\n\ndef prepare_tt_kar_text(json_file_path,pattern=None,lower=False):\n    # this function return dictionary \n    with open(json_file_path,'r') as file:\n        data=json.loads(file.read())\n    dict_data=defaultdict(lambda: defaultdict(list))\n    for example in data['images']:\n        temp=[]\n        for sentence in example['sentences']:\n            cap=sentence['raw']\n            if lower:\n                cap=cap.lower()\n                \n            cap=cap.translate(str.maketrans('','',string.punctuation))\n            \n            if pattern is not None:\n                cap=re.sub(pattern,'',cap)\n            cap=' '.join(cap.split())\n            \n            dict_data[example['split']][example['filename']].append(cap)\n\n            \n# {'train':[{'image':'name.jpg','image_id':img_id,'caption':cap}],'test':[{'image':'name.jpg','captions':[cap1,...,cap5]}],'val':[{'image':'name.jpg','captions':[cap1,...,cap5]}]}  \n    return dict_data\nkar_with_pattern=prepare_tt_kar_text('/kaggle/input/karpathy-splits/dataset_flickr8k.json',pattern=pattern,lower=True)\nraw_kar=prepare_tt_kar_text('/kaggle/input/karpathy-splits/dataset_flickr8k.json',pattern=None,lower=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-14T02:24:41.740028Z","iopub.execute_input":"2023-04-14T02:24:41.741270Z","iopub.status.idle":"2023-04-14T02:24:43.032302Z","shell.execute_reply.started":"2023-04-14T02:24:41.741212Z","shell.execute_reply":"2023-04-14T02:24:43.031197Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# train_result=json.load(open('/kaggle/input/blip-mask-results-1e/train_result.json','r'))\n# val_result=json.load(open('/kaggle/input/blip-mask-results-1e/val_result.json','r'))\n# test_result=json.load(open('/kaggle/input/blip-mask-results-1e/test_result.json','r'))\n\n# len(train_result),len(val_result),len(test_result),train_result[0],val_result[0],test_result[0]","metadata":{"execution":{"iopub.status.busy":"2023-04-14T01:33:57.219865Z","iopub.status.idle":"2023-04-14T01:33:57.220723Z","shell.execute_reply.started":"2023-04-14T01:33:57.220447Z","shell.execute_reply":"2023-04-14T01:33:57.220491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in raw_kar['train']:\n    print(raw_kar['train'][i])\n    print()\n    print(kar_with_pattern['train'][i])\n    break","metadata":{"execution":{"iopub.status.busy":"2023-04-14T02:24:52.947091Z","iopub.execute_input":"2023-04-14T02:24:52.947542Z","iopub.status.idle":"2023-04-14T02:24:52.953799Z","shell.execute_reply.started":"2023-04-14T02:24:52.947503Z","shell.execute_reply":"2023-04-14T02:24:52.952687Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"['A black dog is running after a white dog in the snow', 'Black dog chasing brown dog through snow', 'Two dogs chase each other across the snowy ground', 'Two dogs play together in the snow', 'Two dogs running through a low lying body of water']\n\n['a black dog running after a white dog in the snow', 'black dog chasing brown dog through snow', 'two dogs chase each other across the snowy ground', 'two dogs play together in the snow', 'two dogs running through a low lying body water']\n","output_type":"stream"}]},{"cell_type":"code","source":"import csv\ncaptioning_results={\n    'train':json.load(open('/kaggle/working/train_result.json','r')),\n    'val':json.load(open('output/Caption_coco/result/val_epoch0.json','r')),\n    'test':json.load(open('output/Caption_coco/result/test_epoch0.json','r'))\n}\ndef generate_csv(csv_path, splits=['train'],input_mode={'pred':True,'masked':True}):\n    # input_dict: {'img_name':['one caption']}\n    # refs_dict: {'img_name':['caption1','cap2','cap3']}\n    \n    with open(csv_path, 'w', newline='') as csvfile:\n        writter = csv.writer(csvfile)\n        writter.writerow([\"input\", \"target\"])\n        for sp in splits:\n            if input_mode['masked']:\n                for img in kar_with_pattern[sp]:\n                    for i in range(len(kar_with_pattern[sp][img])):\n                        writter.writerow(['grammar: '+kar_with_pattern[sp][img][i], raw_kar[sp][img][i]])\n            if input_mode['pred']:\n                for res in captioning_results[sp]:\n                    for cap in raw_kar[sp][res['image_id']]:\n                        writter.writerow(['grammar: '+res['caption'], cap])\n","metadata":{"execution":{"iopub.status.busy":"2023-04-14T02:25:05.267780Z","iopub.execute_input":"2023-04-14T02:25:05.268417Z","iopub.status.idle":"2023-04-14T02:25:05.290418Z","shell.execute_reply.started":"2023-04-14T02:25:05.268379Z","shell.execute_reply":"2023-04-14T02:25:05.289498Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"\ngenerate_csv(\"train.csv\", splits=['train'],input_mode={'pred':True,'masked':True})\ngenerate_csv(\"eval.csv\", splits=['val'],input_mode={'pred':True,'masked':False})\ngenerate_csv(\"test.csv\", splits=['test'],input_mode={'pred':True,'masked':False})","metadata":{"execution":{"iopub.status.busy":"2023-04-14T02:25:12.246305Z","iopub.execute_input":"2023-04-14T02:25:12.247033Z","iopub.status.idle":"2023-04-14T02:25:12.430763Z","shell.execute_reply.started":"2023-04-14T02:25:12.246993Z","shell.execute_reply":"2023-04-14T02:25:12.429628Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n# df=pd.read_csv('train.csv')\n# len(df)","metadata":{"execution":{"iopub.status.busy":"2023-04-14T01:33:57.230544Z","iopub.status.idle":"2023-04-14T01:33:57.231384Z","shell.execute_reply.started":"2023-04-14T01:33:57.231102Z","shell.execute_reply":"2023-04-14T01:33:57.231129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df.head(20)","metadata":{"execution":{"iopub.status.busy":"2023-04-14T01:33:57.232895Z","iopub.status.idle":"2023-04-14T01:33:57.233731Z","shell.execute_reply.started":"2023-04-14T01:33:57.233451Z","shell.execute_reply":"2023-04-14T01:33:57.233495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install happytransformer\n\nfrom happytransformer import HappyTextToText, TTSettings\n\nhappy_tt = HappyTextToText(\"T5\", \"vennify/t5-base-grammar-correction\")\n# happy_tt = HappyTextToText(\"T5\", \"t5-base\")\n\n\nargs = TTSettings(num_beams=5, min_length=1)\n\n# Add the prefix \"grammar: \" before each input \nresult = happy_tt.generate_text(\"grammar: This sentences has has bads grammar.\", args=args)\nprint(result.text) # This sentence has bad grammar.","metadata":{"execution":{"iopub.status.busy":"2023-04-14T02:25:17.845912Z","iopub.execute_input":"2023-04-14T02:25:17.846676Z","iopub.status.idle":"2023-04-14T02:26:22.924932Z","shell.execute_reply.started":"2023-04-14T02:25:17.846637Z","shell.execute_reply":"2023-04-14T02:26:22.923686Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Collecting happytransformer\n  Downloading happytransformer-2.4.1-py3-none-any.whl (45 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: datasets>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from happytransformer) (2.1.0)\nRequirement already satisfied: torch>=1.0 in /opt/conda/lib/python3.7/site-packages (from happytransformer) (1.13.0)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.7/site-packages (from happytransformer) (3.20.3)\nRequirement already satisfied: transformers>=4.4.0 in /opt/conda/lib/python3.7/site-packages (from happytransformer) (4.15.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from happytransformer) (0.1.97)\nRequirement already satisfied: tqdm>=4.43 in /opt/conda/lib/python3.7/site-packages (from happytransformer) (4.64.1)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets>=1.6.0->happytransformer) (5.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets>=1.6.0->happytransformer) (0.3.6)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets>=1.6.0->happytransformer) (1.21.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets>=1.6.0->happytransformer) (3.8.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets>=1.6.0->happytransformer) (0.18.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets>=1.6.0->happytransformer) (0.70.14)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets>=1.6.0->happytransformer) (3.2.0)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets>=1.6.0->happytransformer) (2023.1.0)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets>=1.6.0->happytransformer) (2.28.2)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets>=1.6.0->happytransformer) (0.13.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets>=1.6.0->happytransformer) (23.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets>=1.6.0->happytransformer) (1.3.5)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets>=1.6.0->happytransformer) (4.11.4)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.0->happytransformer) (4.4.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.4.0->happytransformer) (2021.11.10)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.4.0->happytransformer) (6.0)\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.4.0->happytransformer) (0.10.3)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers>=4.4.0->happytransformer) (0.0.53)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers>=4.4.0->happytransformer) (3.9.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=1.6.0->happytransformer) (6.0.4)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=1.6.0->happytransformer) (2.1.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=1.6.0->happytransformer) (22.2.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=1.6.0->happytransformer) (1.8.2)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=1.6.0->happytransformer) (4.0.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=1.6.0->happytransformer) (1.3.3)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=1.6.0->happytransformer) (0.13.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=1.6.0->happytransformer) (1.3.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=1.6.0->happytransformer) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=1.6.0->happytransformer) (2022.12.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=1.6.0->happytransformer) (1.26.14)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets>=1.6.0->happytransformer) (3.11.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets>=1.6.0->happytransformer) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets>=1.6.0->happytransformer) (2023.3)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers>=4.4.0->happytransformer) (1.2.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers>=4.4.0->happytransformer) (1.16.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers>=4.4.0->happytransformer) (8.1.3)\nInstalling collected packages: happytransformer\nSuccessfully installed happytransformer-2.4.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.38k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39a53abe0e9845ca837811afabab0792"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/850M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5963fc86965b434099c9348b08ecadd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.88k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a28430d2eb049d6adb01a26a014fde9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/773k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0500ba4da8aa423a83dc9e71b3827e28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.32M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28dd208592c041dea50243164bcb67ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.74k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"188a4af00da34fae97a9cd24ab953670"}},"metadata":{}},{"name":"stdout","text":"This sentence has bad grammar.\n","output_type":"stream"}]},{"cell_type":"code","source":"from happytransformer import TTTrainArgs\nargs2 = TTTrainArgs(batch_size=50)\nhappy_tt.train(\"train.csv\", args=args2)","metadata":{"execution":{"iopub.status.busy":"2023-04-14T02:26:30.887223Z","iopub.execute_input":"2023-04-14T02:26:30.887924Z","iopub.status.idle":"2023-04-14T02:49:12.752897Z","shell.execute_reply.started":"2023-04-14T02:26:30.887885Z","shell.execute_reply":"2023-04-14T02:49:12.751913Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-4c2273126b3d9aec/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ce39c9b9bbf427b84ff1ee8725ebcf5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ed64a63e1094daa933a132c0188b859"}},"metadata":{}},{"name":"stdout","text":"Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-4c2273126b3d9aec/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aaf2a182ef394a53aa0479a376578859"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/60 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42e8ab3c34484baa8edfd400a548cd71"}},"metadata":{}},{"name":"stderr","text":"***** Running training *****\n  Num examples = 60000\n  Num Epochs = 3\n  Instantaneous batch size per device = 50\n  Total train batch size (w. parallel, distributed & accumulation) = 50\n  Gradient Accumulation steps = 1\n  Total optimization steps = 3600\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3600' max='3600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3600/3600 22:33, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.381800</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.257700</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.199200</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.169800</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>1.181300</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>1.147200</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>1.133700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# saving model","metadata":{}},{"cell_type":"code","source":"happy_tt.save(\"/kaggle/working/happy_tt/\")","metadata":{"execution":{"iopub.status.busy":"2023-04-14T02:49:12.755889Z","iopub.execute_input":"2023-04-14T02:49:12.756290Z","iopub.status.idle":"2023-04-14T02:49:14.766663Z","shell.execute_reply.started":"2023-04-14T02:49:12.756253Z","shell.execute_reply":"2023-04-14T02:49:14.765170Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"Configuration saved in /kaggle/working/happy_tt/config.json\nModel weights saved in /kaggle/working/happy_tt/pytorch_model.bin\ntokenizer config file saved in /kaggle/working/happy_tt/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/happy_tt/special_tokens_map.json\nCopy vocab file to /kaggle/working/happy_tt/spiece.model\n","output_type":"stream"}]},{"cell_type":"code","source":"before_training=happy_tt.eval('eval.csv')\nprint(before_training.loss)","metadata":{"execution":{"iopub.status.busy":"2023-04-14T01:33:57.242350Z","iopub.status.idle":"2023-04-14T01:33:57.243183Z","shell.execute_reply.started":"2023-04-14T01:33:57.242913Z","shell.execute_reply":"2023-04-14T01:33:57.242940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds2=[]\nannot={'annotations':[],'images':[]}\nfor i,res in enumerate(tqdm(captioning_results['test'])):\n    out_cap=happy_tt.generate_text(\"grammar: \"+res['caption'], args=args).text\n    preds2.append({'image_id':res['image_id'],'caption':out_cap})\n    for cap in raw_kar['test'][res['image_id']]:\n        annot['images'].append({'id':res['image_id']})\n        annot['annotations'].append({'image_id':res['image_id'],'caption':cap,'id':i})\n        ","metadata":{"execution":{"iopub.status.busy":"2023-04-14T02:49:14.768499Z","iopub.execute_input":"2023-04-14T02:49:14.768879Z","iopub.status.idle":"2023-04-14T02:53:11.681172Z","shell.execute_reply.started":"2023-04-14T02:49:14.768840Z","shell.execute_reply":"2023-04-14T02:53:11.679956Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed8894dca4964e2a80a486e62aa7bdd9"}},"metadata":{}}]},{"cell_type":"code","source":"json.dump(annot,open('/kaggle/working/coco_karpathy_test_gt.json','w'))\njson.dump(preds2,open('/kaggle/working/coco_karpathy_test_result.json','w'))\ncoco_val = coco_caption_eval('/kaggle/working','/kaggle/working/coco_karpathy_test_result.json','test')","metadata":{"execution":{"iopub.status.busy":"2023-04-14T02:53:11.683481Z","iopub.execute_input":"2023-04-14T02:53:11.683933Z","iopub.status.idle":"2023-04-14T02:54:54.167197Z","shell.execute_reply.started":"2023-04-14T02:53:11.683894Z","shell.execute_reply":"2023-04-14T02:54:54.165539Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"loading annotations into memory...\nDone (t=0.01s)\ncreating index...\nindex created!\nLoading and preparing results...\nDONE (t=0.00s)\ncreating index...\nindex created!\ntokenization...\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"PTBTokenizer tokenized 59208 tokens at 141402.80 tokens per second.\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"PTBTokenizer tokenized 10760 tokens at 54938.75 tokens per second.\n","output_type":"stream"},{"name":"stdout","text":"setting up scorers...\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\ncomputing Bleu score...\n{'testlen': 9761, 'reflen': 9749, 'guess': [9761, 8761, 7761, 6761], 'correct': [7669, 4580, 2410, 1169]}\nratio: 1.0012308954763565\nBleu_1: 0.786\nBleu_2: 0.641\nBleu_3: 0.503\nBleu_4: 0.385\ncomputing METEOR score...\nMETEOR: 0.294\ncomputing Rouge score...\nROUGE_L: 0.599\ncomputing CIDEr score...\nCIDEr: 1.101\ncomputing SPICE score...\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"WARNING: An illegal reflective access operation has occurred\nWARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/opt/conda/lib/python3.7/site-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value\nWARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\nWARNING: All illegal access operations will be denied in a future release\nParsing reference captions\nInitiating Stanford parsing pipeline\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \ndone [1.2 sec].\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\nLoading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.8 sec].\nLoading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [1.1 sec].\nLoading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.9 sec].\nThreads( StanfordCoreNLP ) ","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_24/2107726655.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/working/coco_karpathy_test_gt.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/working/coco_karpathy_test_result.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcoco_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoco_caption_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/working'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'/kaggle/working/coco_karpathy_test_result.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/kaggle/working/Blip/data/utils.py\u001b[0m in \u001b[0;36mcoco_caption_eval\u001b[0;34m(coco_gt_root, results_file, split)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m# evaluate results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;31m# SPICE will take a few minutes the first time, but speeds up due to caching\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mcoco_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;31m# print output evaluation scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pycocoevalcap/eval.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscorers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'computing %s score...'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pycocoevalcap/spice/spice.py\u001b[0m in \u001b[0;36mcompute_score\u001b[0;34m(self, gts, res)\u001b[0m\n\u001b[1;32m     74\u001b[0m         ]\n\u001b[1;32m     75\u001b[0m         subprocess.check_call(spice_cmd, \n\u001b[0;32m---> 76\u001b[0;31m             cwd=os.path.dirname(os.path.abspath(__file__)))\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# Read and process results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0mcheck_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ls\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-l\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \"\"\"\n\u001b[0;32m--> 358\u001b[0;31m     \u001b[0mretcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Including KeyboardInterrupt, wait handled that.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1017\u001b[0m             \u001b[0mendtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1651\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                             \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Another thread waited.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1653\u001b[0;31m                         \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1654\u001b[0m                         \u001b[0;31m# Check the pid and loop as waitpid has been known to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m                         \u001b[0;31m# return 0 even without WNOHANG in odd situations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1609\u001b[0m             \u001b[0;34m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1611\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1612\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mChildProcessError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m                 \u001b[0;31m# This happens if SIGCLD is set to be ignored or waiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"assert(False)","metadata":{"execution":{"iopub.status.busy":"2023-04-14T01:33:57.249758Z","iopub.status.idle":"2023-04-14T01:33:57.250126Z","shell.execute_reply.started":"2023-04-14T01:33:57.249952Z","shell.execute_reply":"2023-04-14T01:33:57.249971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"json.dump(annot,open('/kaggle/working/coco_karpathy_test_gt.json','w'))\njson.dump(preds2,open('/kaggle/working/coco_karpathy_test_result.json','w'))\ncoco_val = coco_caption_eval('/kaggle/working','/kaggle/working/coco_karpathy_test_result.json','test')\n","metadata":{"execution":{"iopub.status.busy":"2023-04-14T01:33:57.252511Z","iopub.status.idle":"2023-04-14T01:33:57.253345Z","shell.execute_reply.started":"2023-04-14T01:33:57.253066Z","shell.execute_reply":"2023-04-14T01:33:57.253092Z"},"trusted":true},"execution_count":null,"outputs":[]}]}